{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS Factor Selection Case Study and Multiple testing\n",
    "\n",
    "This is based on an [article](https://www.venn.twosigma.com/vennsights/article/factor-selection-case-study) by Venn by TWO SIGMA, where relaxed Lasso regression is used for factor selection.\n",
    "\n",
    "_\"We construct 7 explanatory factors of 36 data points each. The factors are completely random values within a normal distribution with a mean of zero and a standard deviation of 2%.\n",
    "We constructed a “noise” data stream that also has 36 data points with random values pulled from a normal distribution with a mean of zero and a standard deviation of 0.5%.\n",
    "We constructed a dependent return stream using 100% of Factor 1 + 50% of Factor 2 + 30% of Factor 3 + “noise” data. The dependent return stream has no relationship to Factors 4 through 7.\"_\n",
    "\n",
    "\n",
    "Below we fit the observed asset returns to the observed factors, calculate the T-statistic for each factor parameter at the 5% confidence level. This amounts to performing the hypothesis test\n",
    "\n",
    "$ \\begin{align}\\mathcal{H}_0: \\beta_i = 0 \\\\ \\mathcal{H}_1: \\beta_i \\neq 0 \\end{align}$\n",
    "\n",
    "for each factor $i=1,...,7$.\n",
    "\n",
    "We then evaluate the risk of false positives (type I errors), and false negatives (type II errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create factors and the asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "nfactors = 7\n",
    "nsamples = 36\n",
    "\n",
    "factor_vol = 0.02 * np.eye(nfactors)\n",
    "noise_vol = 0.005\n",
    "\n",
    "factor_dist = stats.multivariate_normal(cov=factor_vol**2)\n",
    "noise_dist = stats.multivariate_normal(cov=noise_vol**2)\n",
    "\n",
    "# Asset concisting of liner combinations of factors.\n",
    "weights = np.array([1.0, 0.5, 0.30, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "# True hidden asset returns.\n",
    "asset = lambda factor_innov: factor_innov @ weights\n",
    "\n",
    "# Observed noisy asset returns.\n",
    "noisy_asset = lambda factor_innov: asset(factor_innov) + noise_dist.rvs(factor_innov.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive factor hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.30% risk of including at least one incorrect factor (FWER).\n",
      "4.91% of included factors are incorrect (FDR).\n",
      "0.00% risk of excluding a correct factor (FNR).\n"
     ]
    }
   ],
   "source": [
    "# Fit OLS to noisy_asset, return pvalues.\n",
    "def ols_pvalues(factor_innov, asset_innov):\n",
    "    X = sm.add_constant(factor_innov) # Don't forget that intercept (you should never really omit it).\n",
    "    Y = asset_innov\n",
    "    mdl = sm.OLS(Y, X)\n",
    "    result = mdl.fit()\n",
    "    return result.pvalues\n",
    "\n",
    "# Simulate OLS pvalues.\n",
    "def simulate_ols_pvalues(nsim):\n",
    "    pvalues = np.zeros((nsim, 1 + nfactors))\n",
    "    for i in range(nsim):\n",
    "        factor_innov = factor_dist.rvs(nsamples)\n",
    "        asset_innov = noisy_asset(factor_innov)\n",
    "        pvalues[i] = ols_pvalues(factor_innov, asset_innov)\n",
    "    return pvalues\n",
    "\n",
    "\n",
    "import inference\n",
    "np.random.seed(1)\n",
    "nsim = 2000\n",
    "pvals = simulate_ols_pvalues(nsim)\n",
    "alpha = 0.05\n",
    "H0 = (weights == 0.0)\n",
    "stats, _ = inference.multitesting(alpha, pvals[:,1::], H0)\n",
    "print('{:.2f}% risk of including at least one incorrect factor (FWER).'.format(100*stats['FWER']))\n",
    "print('{:.2f}% of included factors are incorrect (FDR).'.format(100*stats['FDR']))\n",
    "print('{:.2f}% risk of excluding a correct factor (FNR).'.format(100*stats['FNR']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation results an 18% risk of including at least one of the \"incorrect\" factors 4-7. That is, we have a 18% risk of rejecting at least one true null hypothesis. Our chosen 5% significant level is the maximum risk of rejecting a true null hypthesis for one parameter. The more hypotheses tests we perform with true null hypotheses, the higher the risk we face of false rejections.\n",
    "\n",
    "In fact, since we have four factors, each with a true null hypothesis, if we assume independent tests, we will include at least one \"false\" factor on average $1-(1-\\alpha)^4 = 18.5\\%$ of the time. However, since in reality, we do not know the truth of the seven hypotheses, we must expect to falsely including at least one factor around $1-(1-\\alpha)^7 = 1/3$ of the times.\n",
    "Hence, when performing multiple hypotheses tests, we must chose a more conservative confidence if we want to avoid false positives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiple factor hypothesis testing\n",
    "\n",
    "Assume that we have $m$ __independent__ hypotheses tests with corresponding p-values, where the null hypotheses are known a priori to be true.\n",
    "Then the probability of observing no rejections is $(1-\\alpha)^n$, and hence the probability of observing at least one false rejection is $1-(1-\\alpha)^m$.\n",
    "\n",
    "We call this confidence level for the 'family' of hypotheses tests the family-wise error rate (FWER). It can be seen as extending the usual $\\alpha$ value for multiple tests. Hence, if we want a combined confidence level of $\\alpha$, we could modify our original confidence level to give a maximum 5% risk of rejecting at least one true null hypothesis. Equivalently, we can modify the p-value directly.\n",
    "\n",
    "The above procedure is called the Šidák correction, and it aims to control the probability of false discoveries, or type I error of rejecting at least one true null hypothesis. However, it is important to understand that lowering the confidence level means a sacrifice of statistical power, an increased risk of false negatives, or type II errors. Also, since the Šidák correction assumes independence, it will be conservative for tests that are positively dependent, and liberal for negatively dependent tests. Therefore, there are many other more or less powerful corrections, with various assumption on independence.\n",
    "\n",
    "While some corrections aims at controlling the probability (FWER) of false positives, others aims at controling the expected proportion of false positives, the false discovery rate (FDR). Per definition, FWER methods are more conservative with the trade-off of lower statistical power than FDR preocedures.\n",
    "\n",
    "### 3.1. Family-wise error rate\n",
    "\n",
    "$FWER = P(\\text{\"reject at least one true null hypothesis\"}) = P(N_{FP} > 0)$,\n",
    "\n",
    "where $N_{FP}$ is the number of false positives in the family of tests.\n",
    "\n",
    "Learn more @  https://en.wikipedia.org/wiki/Family-wise_error_rate#Controlling_procedures\n",
    "\n",
    "[Read more...](https://en.wikipedia.org/wiki/Family-wise_error_rate#Controlling_procedures)\n",
    "\n",
    "#### 3.1.1. Bonferroni correction\n",
    "\n",
    "For each hypothesis test:\n",
    "reject $\\mathcal{H}_0$ if\n",
    "$p_i \\leq \\alpha / m \\iff mp_i \\leq \\alpha$.\n",
    "\n",
    "Conservative. No assumptions. Also controls the FDR.\n",
    "\n",
    "[Read more...](https://en.wikipedia.org/wiki/Bonferroni_correction)\n",
    "\n",
    "#### 3.1.2. Šidák correction\n",
    "\n",
    "For each hypothesis test:\n",
    "reject $\\mathcal{H}_0$ if\n",
    "$p_i \\leq 1 - (1-\\alpha)^{1/n} \\iff 1- (1-p_i)^n \\leq \\alpha$.\n",
    "\n",
    "More powerful than Benferroni. Assumes independence, too liberal when negative dependence among hypotheses.\n",
    "\n",
    "[Read more...](https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction)\n",
    "\n",
    "#### 3.1.3. Holm–Bonferroni method\n",
    "\n",
    "Order the p-values $p_{(1)}, ..., p_{(m)}$ with associate hypotheses $\\mathcal{H}_{(1)}, ..., \\mathcal{H}_{(m)}$.\n",
    "\n",
    "Let $\\min\\{k \\in \\mathbb{N}\\ | p_{(k)} \\leq \\alpha / (m+1-k)\\}$.\n",
    "\n",
    "Reject $\\mathcal{H}_{(1)}, ..., \\mathcal{H}_{(k-1)}$, and do not reject the rest.\n",
    "\n",
    "At least as powerful as Bonferroni. No assumptions.\n",
    "\n",
    "[Read more...](https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method)\n",
    "\n",
    "### 3.2. False discovery rate\n",
    "\n",
    "$FDR = E(\\text{\"proportion of rejections that are false rejections\"}) = E\\left(\\frac{N_{FP}}{N_{FP} + N_{TP}} > 0 \\right)$.\n",
    "\n",
    "FWER control exerts a more stringent control over false discovery compared to false discovery rate (FDR) procedures. FWER control limits the probability of at least one false discovery, whereas FDR control limits (in a loose sense) the expected proportion of false discoveries. Thus, FDR procedures have greater power at the cost of increased rates of type I errors, i.e., rejecting null hypotheses that are actually true.\n",
    "\n",
    "Some of the methods are the Benjamini–Hochberg, and Benjamini–Yekutieli procedures.\n",
    "\n",
    "[Read more...](https://en.wikipedia.org/wiki/False_discovery_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://jpktd.blogspot.com/2013/04/multiple-testing-p-value-corrections-in.html\n",
    "https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/18/lecture-18.pdf\n",
    "(http://mezeylab.cb.bscb.cornell.edu/labmembers/documents/supplement%205%20-%20multiple%20regression.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us try them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Raw  Bonferroni     Sidak      Holm    fdr_bh\n",
      "FWER  0.183000    0.026500  0.028000  0.048500  0.105500\n",
      "FDR   0.049075    0.006700  0.007075  0.012467  0.028550\n",
      "FNR   0.000000    0.000833  0.000833  0.000667  0.000167\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dfs = []\n",
    "for procedure in ['Raw', 'Bonferroni', 'Sidak', 'Holm', 'fdr_bh']: # (Holm implementation is slow)\n",
    "    stats, _ = inference.multitesting(alpha, pvals[:,1::], H0, procedure=procedure)\n",
    "    dfs += [ pd.DataFrame.from_dict(stats, orient='index', columns=[procedure]) ]\n",
    "\n",
    "df = pd.concat(dfs, axis=1)\n",
    "df = df.loc[['FWER', 'FDR', 'FNR']]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benferroni, Sidak, and Holm all control the risk of false positives (FWER), to try to get it down to the $\\leq 5\\%$ level.\n",
    "Bonferroni and Sidak are conservative at 2.8%, while Holm-Benferroni is closer to the 5% target. Notice how Benjamini/Hochberg has reduced FWER to 11%, since it does not control FWER, but rather the FDR. In terms of statitical power, which we measure in low False Negative Rate, FNR(Holm) $\\leq$ FNR(Sidak) $\\leq$ FNR(Bonferroni) $\\leq$ FNR(fdr_bh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
